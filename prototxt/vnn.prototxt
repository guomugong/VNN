name: "vnn"

#layer {
#  name: "data"
#  type: "ImageSegData"
#  top: "data"
#  top: "label"
#  transform_param {
#    mirror: true
#    crop_size: 632
#  }
#  image_data_param {
#    root_folder: "/home/guo/dataset/chase_db1/"
#    source: "./data/chase_db1/train_sample3.lst"
##   source: "./data/chase_db1/train_sample_all.lst"
#    batch_size: 1
#    shuffle: true
#    label_type: PIXEL #0, 1
#  }
#}

layer {
  name: "data"
  type: "ImageSegData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 512
  }
  image_data_param {
    root_folder: "/home/guo/dataset/drive/"
    source: "./data/drive/train_sample_all.lst"
    batch_size: 1
    shuffle: true
    label_type: PIXEL #0, 1
  }
}


# head
layer { name: 'conv0' bottom: 'data' top: 'conv0' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 32 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'relu0' bottom: 'conv0' top: 'conv0' type: "ReLU" }


# subnet
layer { name: "conv0_down8" type: "Pooling" bottom: "conv0" top: "conv0_down8"
  pooling_param { pool: MAX kernel_size: 12 stride: 8 } }

# GDP-Block (Gated Detail-Preserving Block)
##block 9, kernel{16, 16, 16}
#branch 1
layer { name: "bl9_b1_conv" bottom: "conv0_down8" top: "bl9_b1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_b1_relu" bottom: "bl9_b1_conv" top: "bl9_b1_conv" type: "ReLU" }

#branch 2
layer { name: "bl9_b2_pool" bottom: "conv0_down8" top: "bl9_b2_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: "bl9_b2_conv" bottom: "bl9_b2_pool" top: "bl9_b2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_b2_relu" bottom: "bl9_b2_conv" top: "bl9_b2_conv" type: "ReLU" }

#branch 3
layer { name: "bl9_b3_pool" bottom: "conv0_down8" top: "bl9_b3_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: "bl9_b3_conv" bottom: "bl9_b3_pool" top: "bl9_b3_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_b3_relu" bottom: "bl9_b3_conv" top: "bl9_b3_conv" type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl9_b3_up2" type: "Deconvolution"  bottom: "bl9_b3_conv" top: "bl9_b3_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl9_b3_crop" type: "Crop" bottom: "bl9_b3_up2" bottom: "bl9_b2_conv" top: "bl9_b3_crop" crop_param {axis: 2 offset: 1 }}
layer { name: "bl9_b23_fusion" bottom: "bl9_b3_crop" bottom: "bl9_b2_conv" top: "bl9_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl9_b23_output" bottom: "bl9_b23_fusion" top: "bl9_b23_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_b23_output_relu" bottom: "bl9_b23_output" top: "bl9_b23_output" type: "ReLU" }

# attention: branch 23
layer { name: "bl9_atten1_conv" bottom: "bl9_b23_output" top: "bl9_atten1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl9_atten1_relu" bottom: "bl9_atten1_conv" top: "bl9_atten1_conv" type: "ReLU" }
layer { name: "bl9_atten1_weight" bottom: "bl9_atten1_conv" top: "bl9_atten1_weight" type: "Sigmoid" }
layer { name: "bl9_b23_atten_output" bottom: "bl9_b23_output" bottom: "bl9_atten1_weight" 
	top: "bl9_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl9_b23_output
layer { name: "bl9_b23_output_up2" type: "Deconvolution"  bottom: "bl9_atten1_output" top: "bl9_b23_output_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl9_b23_output_up2_crop" type: "Crop" bottom: "bl9_b23_output_up2" bottom: "bl9_b1_conv" top: "bl9_b23_output_up2_crop" crop_param {axis: 2 offset: 1 }}

layer { name: "bl9_b123_fusion" bottom: "bl9_b23_output_up2_crop" bottom: "bl9_b1_conv" top: "bl9_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl9_b123_output" bottom: "bl9_b123_fusion" top: "bl9_b123_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_b123_output_relu" bottom: "bl9_b123_output" top: "bl9_b123_output" type: "ReLU" }

# attention: branch 123
layer { name: "bl9_atten2_conv" bottom: "bl9_b123_output" top: "bl9_atten2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl9_atten2_relu" bottom: "bl9_atten2_conv" top: "bl9_atten2_conv" type: "ReLU" }
layer { name: "bl9_atten2_weight" bottom: "bl9_atten2_conv" top: "bl9_atten2_weight" type: "Sigmoid" }
layer { name: "bl9_b123_atten_output" bottom: "bl9_b123_output" bottom: "bl9_atten2_weight" 
	top: "bl9_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl9_res_sum" bottom: "bl9_atten2_output" bottom: "conv0_down8" top: "bl9_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: "bl9_res_output" bottom: "bl9_res_sum" top: "bl9_res_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl9_res_output_relu" bottom: "bl9_res_output" top: "bl9_res_output" type: "ReLU" }


##block 10, kernel{16, 16, 16}
#branch 1
layer { name: "bl10_b1_conv" bottom: "bl9_res_output" top: "bl10_b1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_b1_relu" bottom: "bl10_b1_conv" top: "bl10_b1_conv" type: "ReLU" }

#branch 2
layer { name: "bl10_b2_pool" bottom: "bl9_res_output" top: "bl10_b2_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: "bl10_b2_conv" bottom: "bl10_b2_pool" top: "bl10_b2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_b2_relu" bottom: "bl10_b2_conv" top: "bl10_b2_conv" type: "ReLU" }

#branch 3
layer { name: "bl10_b3_pool" bottom: "bl9_res_output" top: "bl10_b3_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: "bl10_b3_conv" bottom: "bl10_b3_pool" top: "bl10_b3_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_b3_relu" bottom: "bl10_b3_conv" top: "bl10_b3_conv" type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl10_b3_up2" type: "Deconvolution"  bottom: "bl10_b3_conv" top: "bl10_b3_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl10_b3_crop" type: "Crop" bottom: "bl10_b3_up2" bottom: "bl10_b2_conv" top: "bl10_b3_crop" crop_param {axis: 2 offset: 1 }}
layer { name: "bl10_b23_fusion" bottom: "bl10_b3_crop" bottom: "bl10_b2_conv" top: "bl10_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl10_b23_output" bottom: "bl10_b23_fusion" top: "bl10_b23_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_b23_output_relu" bottom: "bl10_b23_output" top: "bl10_b23_output" type: "ReLU" }

# attention: branch 23
layer { name: "bl10_atten1_conv" bottom: "bl10_b23_output" top: "bl10_atten1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl10_atten1_relu" bottom: "bl10_atten1_conv" top: "bl10_atten1_conv" type: "ReLU" }
layer { name: "bl10_atten1_weight" bottom: "bl10_atten1_conv" top: "bl10_atten1_weight" type: "Sigmoid" }
layer { name: "bl10_b23_atten_output" bottom: "bl10_b23_output" bottom: "bl10_atten1_weight" 
	top: "bl10_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl10_b23_output
layer { name: "bl10_b23_output_up2" type: "Deconvolution"  bottom: "bl10_atten1_output" top: "bl10_b23_output_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl10_b23_output_up2_crop" type: "Crop" bottom: "bl10_b23_output_up2" bottom: "bl10_b1_conv" top: "bl10_b23_output_up2_crop" crop_param {axis: 2 offset: 1 }}

layer { name: "bl10_b123_fusion" bottom: "bl10_b23_output_up2_crop" bottom: "bl10_b1_conv" top: "bl10_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl10_b123_output" bottom: "bl10_b123_fusion" top: "bl10_b123_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_b123_output_relu" bottom: "bl10_b123_output" top: "bl10_b123_output" type: "ReLU" }

# attention: branch 123
layer { name: "bl10_atten2_conv" bottom: "bl10_b123_output" top: "bl10_atten2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl10_atten2_relu" bottom: "bl10_atten2_conv" top: "bl10_atten2_conv" type: "ReLU" }
layer { name: "bl10_atten2_weight" bottom: "bl10_atten2_conv" top: "bl10_atten2_weight" type: "Sigmoid" }
layer { name: "bl10_b123_atten_output" bottom: "bl10_b123_output" bottom: "bl10_atten2_weight" 
	top: "bl10_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl10_res_sum" bottom: "bl10_atten2_output" bottom: "bl9_res_output" 
	top: "bl10_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: "bl10_res_output" bottom: "bl10_res_sum" top: "bl10_res_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl10_res_output_relu" bottom: "bl10_res_output" top: "bl10_res_output" type: "ReLU" }


##block 11, kernel{16, 16, 16}
#branch 1
layer { name: "bl11_b1_conv" bottom: "bl10_res_output" top: "bl11_b1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_b1_relu" bottom: "bl11_b1_conv" top: "bl11_b1_conv" type: "ReLU" }

#branch 2
layer { name: "bl11_b2_pool" bottom: "bl10_res_output" top: "bl11_b2_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: "bl11_b2_conv" bottom: "bl11_b2_pool" top: "bl11_b2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_b2_relu" bottom: "bl11_b2_conv" top: "bl11_b2_conv" type: "ReLU" }

#branch 3
layer { name: "bl11_b3_pool" bottom: "bl10_res_output" top: "bl11_b3_pool" type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: "bl11_b3_conv" bottom: "bl11_b3_pool" top: "bl11_b3_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_b3_relu" bottom: "bl11_b3_conv" top: "bl11_b3_conv" type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl11_b3_up2" type: "Deconvolution"  bottom: "bl11_b3_conv" top: "bl11_b3_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl11_b3_crop" type: "Crop" bottom: "bl11_b3_up2" bottom: "bl11_b2_conv" top: "bl11_b3_crop" crop_param {axis: 2 offset: 1 }}
layer { name: "bl11_b23_fusion" bottom: "bl11_b3_crop" bottom: "bl11_b2_conv" top: "bl11_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl11_b23_output" bottom: "bl11_b23_fusion" top: "bl11_b23_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_b23_output_relu" bottom: "bl11_b23_output" top: "bl11_b23_output" type: "ReLU" }

# attention: branch 23
layer { name: "bl11_atten1_conv" bottom: "bl11_b23_output" top: "bl11_atten1_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl11_atten1_relu" bottom: "bl11_atten1_conv" top: "bl11_atten1_conv" type: "ReLU" }
layer { name: "bl11_atten1_weight" bottom: "bl11_atten1_conv" top: "bl11_atten1_weight" type: "Sigmoid" }
layer { name: "bl11_b23_atten_output" bottom: "bl11_b23_output" bottom: "bl11_atten1_weight" 
	top: "bl11_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl11_b23_output
layer { name: "bl11_b23_output_up2" type: "Deconvolution"  bottom: "bl11_atten1_output" top: "bl11_b23_output_up2"
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 16  weight_filler {type:"bilinear"}} }
layer { name: "bl11_b23_output_up2_crop" type: "Crop" bottom: "bl11_b23_output_up2" bottom: "bl11_b1_conv" top: "bl11_b23_output_up2_crop" crop_param {axis: 2 offset: 1 }}

layer { name: "bl11_b123_fusion" bottom: "bl11_b23_output_up2_crop" bottom: "bl11_b1_conv" top: "bl11_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: "bl11_b123_output" bottom: "bl11_b123_fusion" top: "bl11_b123_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_b123_output_relu" bottom: "bl11_b123_output" top: "bl11_b123_output" type: "ReLU" }

# attention: branch 123
layer { name: "bl11_atten2_conv" bottom: "bl11_b123_output" top: "bl11_atten2_conv" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: "bl11_atten2_relu" bottom: "bl11_atten2_conv" top: "bl11_atten2_conv" type: "ReLU" }
layer { name: "bl11_atten2_weight" bottom: "bl11_atten2_conv" top: "bl11_atten2_weight" type: "Sigmoid" }
layer { name: "bl11_b123_atten_output" bottom: "bl11_b123_output" bottom: "bl11_atten2_weight" 
	top: "bl11_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl11_res_sum" bottom: "bl11_atten2_output" bottom: "bl10_res_output" top: "bl11_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: "bl11_res_output" bottom: "bl11_res_sum" top: "bl11_res_output" type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: "bl11_res_output_relu" bottom: "bl11_res_output" top: "bl11_res_output" type: "ReLU" }

layer { name: "subnet_up8" type: "Deconvolution" bottom: "bl11_res_output" top: "subnet_up8"
  param { lr_mult: 0 decay_mult: 1 } param { lr_mult: 0 decay_mult: 0 }
  convolution_param { num_output: 16 kernel_size: 16 stride: 8 weight_filler { type: "bilinear" } } }
layer { name: "subnet_up8_crop" type: "Crop" bottom: "subnet_up8" bottom: "data" top: "subnet_upscore" }

layer { name: "subnet_score" type: "Convolution" bottom: "subnet_upscore" top: "subnet_score"
  param { lr_mult:0.1 decay_mult: 1 } param { lr_mult: 0.2 decay_mult: 0 }
  convolution_param { num_output: 1 kernel_size: 1 weight_filler { type: "xavier" } } }

layer { type: "SigmoidWeightedCrossEntropyLoss" bottom: "subnet_score" bottom: "label" top: "subnet_loss" loss_weight: 1 }

##### End of SubNet


# GDP-Block (Gated Detail-Preserving Block)
##block 1, kernel{16, 8, 8}
#branch 1
layer { name: 'bl1_b1_conv' bottom: 'conv0' top: 'bl1_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_b1_relu' bottom: 'bl1_b1_conv' top: 'bl1_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl1_b2_pool' bottom: 'conv0' top: 'bl1_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl1_b2_conv' bottom: 'bl1_b2_pool' top: 'bl1_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_b2_relu' bottom: 'bl1_b2_conv' top: 'bl1_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl1_b3_pool' bottom: 'conv0' top: 'bl1_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl1_b3_conv' bottom: 'bl1_b3_pool' top: 'bl1_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_b3_relu' bottom: 'bl1_b3_conv' top: 'bl1_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl1_b3_up2" type: "Deconvolution"  bottom: 'bl1_b3_conv' top: 'bl1_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl1_b3_crop" type: "Crop" bottom: 'bl1_b3_up2' bottom: 'bl1_b2_conv' top: 'bl1_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl1_b23_fusion" bottom: "bl1_b3_crop" bottom: "bl1_b2_conv" top: "bl1_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl1_b23_output' bottom: 'bl1_b23_fusion' top: 'bl1_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_b23_output_relu' bottom: 'bl1_b23_output' top: 'bl1_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl1_atten1_conv' bottom: 'bl1_b23_output' top: 'bl1_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl1_atten1_relu' bottom: 'bl1_atten1_conv' top: 'bl1_atten1_conv' type: "ReLU" }
layer { name: 'bl1_atten1_weight' bottom: 'bl1_atten1_conv' top: 'bl1_atten1_weight' type: "Sigmoid" }
layer { name: "bl1_b23_atten_output" bottom: "bl1_b23_output" bottom: "bl1_atten1_weight" 
	top: "bl1_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl1_b23_output
layer { name: "bl1_b23_output_up2" type: "Deconvolution"  bottom: 'bl1_atten1_output' top: 'bl1_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl1_b23_output_up2_crop" type: "Crop" bottom: 'bl1_b23_output_up2' bottom: 'bl1_b1_conv' top: 'bl1_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl1_b123_fusion" bottom: "bl1_b23_output_up2_crop" bottom: "bl1_b1_conv" top: "bl1_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl1_b123_output' bottom: 'bl1_b123_fusion' top: 'bl1_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_b123_output_relu' bottom: 'bl1_b123_output' top: 'bl1_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl1_atten2_conv' bottom: 'bl1_b123_output' top: 'bl1_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl1_atten2_relu' bottom: 'bl1_atten2_conv' top: 'bl1_atten2_conv' type: "ReLU" }
layer { name: 'bl1_atten2_weight' bottom: 'bl1_atten2_conv' top: 'bl1_atten2_weight' type: "Sigmoid" }
layer { name: "bl1_b123_atten_output" bottom: "bl1_b123_output" bottom: "bl1_atten2_weight" 
	top: "bl1_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl1_res_sum" bottom: "bl1_atten2_output" bottom: "conv0" top: "bl1_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl1_res_output' bottom: 'bl1_res_sum' top: 'bl1_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl1_res_output_relu' bottom: 'bl1_res_output' top: 'bl1_res_output' type: "ReLU" }


##block 2, kernel{16, 8, 8}
#branch 1
layer { name: 'bl2_b1_conv' bottom: 'bl1_res_output' top: 'bl2_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_b1_relu' bottom: 'bl2_b1_conv' top: 'bl2_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl2_b2_pool' bottom: 'bl1_res_output' top: 'bl2_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl2_b2_conv' bottom: 'bl2_b2_pool' top: 'bl2_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_b2_relu' bottom: 'bl2_b2_conv' top: 'bl2_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl2_b3_pool' bottom: 'bl1_res_output' top: 'bl2_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl2_b3_conv' bottom: 'bl2_b3_pool' top: 'bl2_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_b3_relu' bottom: 'bl2_b3_conv' top: 'bl2_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl2_b3_up2" type: "Deconvolution"  bottom: 'bl2_b3_conv' top: 'bl2_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl2_b3_crop" type: "Crop" bottom: 'bl2_b3_up2' bottom: 'bl2_b2_conv' top: 'bl2_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl2_b23_fusion" bottom: "bl2_b3_crop" bottom: "bl2_b2_conv" top: "bl2_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl2_b23_output' bottom: 'bl2_b23_fusion' top: 'bl2_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_b23_output_relu' bottom: 'bl2_b23_output' top: 'bl2_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl2_atten1_conv' bottom: 'bl2_b23_output' top: 'bl2_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl2_atten1_relu' bottom: 'bl2_atten1_conv' top: 'bl2_atten1_conv' type: "ReLU" }
layer { name: 'bl2_atten1_weight' bottom: 'bl2_atten1_conv' top: 'bl2_atten1_weight' type: "Sigmoid" }
layer { name: "bl2_b23_atten_output" bottom: "bl2_b23_output" bottom: "bl2_atten1_weight" 
	top: "bl2_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl2_b23_output
layer { name: "bl2_b23_output_up2" type: "Deconvolution"  bottom: 'bl2_atten1_output' top: 'bl2_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl2_b23_output_up2_crop" type: "Crop" bottom: 'bl2_b23_output_up2' bottom: 'bl2_b1_conv' top: 'bl2_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl2_b123_fusion" bottom: "bl2_b23_output_up2_crop" bottom: "bl2_b1_conv" top: "bl2_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl2_b123_output' bottom: 'bl2_b123_fusion' top: 'bl2_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_b123_output_relu' bottom: 'bl2_b123_output' top: 'bl2_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl2_atten2_conv' bottom: 'bl2_b123_output' top: 'bl2_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl2_atten2_relu' bottom: 'bl2_atten2_conv' top: 'bl2_atten2_conv' type: "ReLU" }
layer { name: 'bl2_atten2_weight' bottom: 'bl2_atten2_conv' top: 'bl2_atten2_weight' type: "Sigmoid" }
layer { name: "bl2_b123_atten_output" bottom: "bl2_b123_output" bottom: "bl2_atten2_weight" 
	top: "bl2_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl2_res_sum" bottom: "bl2_atten2_output" bottom: "bl1_res_output" bottom: "subnet_score"
	top: "bl2_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl2_res_output' bottom: 'bl2_res_sum' top: 'bl2_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl2_res_output_relu' bottom: 'bl2_res_output' top: 'bl2_res_output' type: "ReLU" }


##block 3, kernel{16, 8, 8}
#branch 1
layer { name: 'bl3_b1_conv' bottom: 'bl2_res_output' top: 'bl3_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_b1_relu' bottom: 'bl3_b1_conv' top: 'bl3_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl3_b2_pool' bottom: 'bl2_res_output' top: 'bl3_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl3_b2_conv' bottom: 'bl3_b2_pool' top: 'bl3_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_b2_relu' bottom: 'bl3_b2_conv' top: 'bl3_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl3_b3_pool' bottom: 'bl2_res_output' top: 'bl3_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl3_b3_conv' bottom: 'bl3_b3_pool' top: 'bl3_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_b3_relu' bottom: 'bl3_b3_conv' top: 'bl3_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl3_b3_up2" type: "Deconvolution"  bottom: 'bl3_b3_conv' top: 'bl3_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl3_b3_crop" type: "Crop" bottom: 'bl3_b3_up2' bottom: 'bl3_b2_conv' top: 'bl3_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl3_b23_fusion" bottom: "bl3_b3_crop" bottom: "bl3_b2_conv" top: "bl3_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl3_b23_output' bottom: 'bl3_b23_fusion' top: 'bl3_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_b23_output_relu' bottom: 'bl3_b23_output' top: 'bl3_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl3_atten1_conv' bottom: 'bl3_b23_output' top: 'bl3_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl3_atten1_relu' bottom: 'bl3_atten1_conv' top: 'bl3_atten1_conv' type: "ReLU" }
layer { name: 'bl3_atten1_weight' bottom: 'bl3_atten1_conv' top: 'bl3_atten1_weight' type: "Sigmoid" }
layer { name: "bl3_b23_atten_output" bottom: "bl3_b23_output" bottom: "bl3_atten1_weight" 
	top: "bl3_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl3_b23_output
layer { name: "bl3_b23_output_up2" type: "Deconvolution"  bottom: 'bl3_atten1_output' top: 'bl3_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl3_b23_output_up2_crop" type: "Crop" bottom: 'bl3_b23_output_up2' bottom: 'bl3_b1_conv' top: 'bl3_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl3_b123_fusion" bottom: "bl3_b23_output_up2_crop" bottom: "bl3_b1_conv" top: "bl3_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl3_b123_output' bottom: 'bl3_b123_fusion' top: 'bl3_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_b123_output_relu' bottom: 'bl3_b123_output' top: 'bl3_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl3_atten2_conv' bottom: 'bl3_b123_output' top: 'bl3_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl3_atten2_relu' bottom: 'bl3_atten2_conv' top: 'bl3_atten2_conv' type: "ReLU" }
layer { name: 'bl3_atten2_weight' bottom: 'bl3_atten2_conv' top: 'bl3_atten2_weight' type: "Sigmoid" }
layer { name: "bl3_b123_atten_output" bottom: "bl3_b123_output" bottom: "bl3_atten2_weight" 
	top: "bl3_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl3_res_sum" bottom: "bl3_atten2_output" bottom: "bl2_res_output" top: "bl3_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl3_res_output' bottom: 'bl3_res_sum' top: 'bl3_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl3_res_output_relu' bottom: 'bl3_res_output' top: 'bl3_res_output' type: "ReLU" }


##block 4, kernel{16, 8, 8}
#branch 1
layer { name: 'bl4_b1_conv' bottom: 'bl3_res_output' top: 'bl4_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_b1_relu' bottom: 'bl4_b1_conv' top: 'bl4_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl4_b2_pool' bottom: 'bl3_res_output' top: 'bl4_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl4_b2_conv' bottom: 'bl4_b2_pool' top: 'bl4_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_b2_relu' bottom: 'bl4_b2_conv' top: 'bl4_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl4_b3_pool' bottom: 'bl3_res_output' top: 'bl4_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl4_b3_conv' bottom: 'bl4_b3_pool' top: 'bl4_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_b3_relu' bottom: 'bl4_b3_conv' top: 'bl4_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl4_b3_up2" type: "Deconvolution"  bottom: 'bl4_b3_conv' top: 'bl4_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl4_b3_crop" type: "Crop" bottom: 'bl4_b3_up2' bottom: 'bl4_b2_conv' top: 'bl4_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl4_b23_fusion" bottom: "bl4_b3_crop" bottom: "bl4_b2_conv" top: "bl4_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl4_b23_output' bottom: 'bl4_b23_fusion' top: 'bl4_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_b23_output_relu' bottom: 'bl4_b23_output' top: 'bl4_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl4_atten1_conv' bottom: 'bl4_b23_output' top: 'bl4_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl4_atten1_relu' bottom: 'bl4_atten1_conv' top: 'bl4_atten1_conv' type: "ReLU" }
layer { name: 'bl4_atten1_weight' bottom: 'bl4_atten1_conv' top: 'bl4_atten1_weight' type: "Sigmoid" }
layer { name: "bl4_b23_atten_output" bottom: "bl4_b23_output" bottom: "bl4_atten1_weight" 
	top: "bl4_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl4_b23_output
layer { name: "bl4_b23_output_up2" type: "Deconvolution"  bottom: 'bl4_atten1_output' top: 'bl4_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl4_b23_output_up2_crop" type: "Crop" bottom: 'bl4_b23_output_up2' bottom: 'bl4_b1_conv' top: 'bl4_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl4_b123_fusion" bottom: "bl4_b23_output_up2_crop" bottom: "bl4_b1_conv" top: "bl4_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl4_b123_output' bottom: 'bl4_b123_fusion' top: 'bl4_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_b123_output_relu' bottom: 'bl4_b123_output' top: 'bl4_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl4_atten2_conv' bottom: 'bl4_b123_output' top: 'bl4_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl4_atten2_relu' bottom: 'bl4_atten2_conv' top: 'bl4_atten2_conv' type: "ReLU" }
layer { name: 'bl4_atten2_weight' bottom: 'bl4_atten2_conv' top: 'bl4_atten2_weight' type: "Sigmoid" }
layer { name: "bl4_b123_atten_output" bottom: "bl4_b123_output" bottom: "bl4_atten2_weight" 
	top: "bl4_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl4_res_sum" bottom: "bl4_atten2_output" bottom: "bl3_res_output" bottom: "subnet_score"
	top: "bl4_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl4_res_output' bottom: 'bl4_res_sum' top: 'bl4_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl4_res_output_relu' bottom: 'bl4_res_output' top: 'bl4_res_output' type: "ReLU" }

##block 5, kernel{16, 8, 8}
#branch 1
layer { name: 'bl5_b1_conv' bottom: 'bl4_res_output' top: 'bl5_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_b1_relu' bottom: 'bl5_b1_conv' top: 'bl5_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl5_b2_pool' bottom: 'bl4_res_output' top: 'bl5_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl5_b2_conv' bottom: 'bl5_b2_pool' top: 'bl5_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_b2_relu' bottom: 'bl5_b2_conv' top: 'bl5_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl5_b3_pool' bottom: 'bl4_res_output' top: 'bl5_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl5_b3_conv' bottom: 'bl5_b3_pool' top: 'bl5_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_b3_relu' bottom: 'bl5_b3_conv' top: 'bl5_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl5_b3_up2" type: "Deconvolution"  bottom: 'bl5_b3_conv' top: 'bl5_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl5_b3_crop" type: "Crop" bottom: 'bl5_b3_up2' bottom: 'bl5_b2_conv' top: 'bl5_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl5_b23_fusion" bottom: "bl5_b3_crop" bottom: "bl5_b2_conv" top: "bl5_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl5_b23_output' bottom: 'bl5_b23_fusion' top: 'bl5_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_b23_output_relu' bottom: 'bl5_b23_output' top: 'bl5_b23_output' type: "ReLU" }


# attention: branch 23
layer { name: 'bl5_atten1_conv' bottom: 'bl5_b23_output' top: 'bl5_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl5_atten1_relu' bottom: 'bl5_atten1_conv' top: 'bl5_atten1_conv' type: "ReLU" }
layer { name: 'bl5_atten1_weight' bottom: 'bl5_atten1_conv' top: 'bl5_atten1_weight' type: "Sigmoid" }
layer { name: "bl5_b23_atten_output" bottom: "bl5_b23_output" bottom: "bl5_atten1_weight" 
	top: "bl5_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl5_b23_output
layer { name: "bl5_b23_output_up2" type: "Deconvolution"  bottom: 'bl5_atten1_output' top: 'bl5_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl5_b23_output_up2_crop" type: "Crop" bottom: 'bl5_b23_output_up2' bottom: 'bl5_b1_conv' top: 'bl5_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl5_b123_fusion" bottom: "bl5_b23_output_up2_crop" bottom: "bl5_b1_conv" top: "bl5_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl5_b123_output' bottom: 'bl5_b123_fusion' top: 'bl5_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_b123_output_relu' bottom: 'bl5_b123_output' top: 'bl5_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl5_atten2_conv' bottom: 'bl5_b123_output' top: 'bl5_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl5_atten2_relu' bottom: 'bl5_atten2_conv' top: 'bl5_atten2_conv' type: "ReLU" }
layer { name: 'bl5_atten2_weight' bottom: 'bl5_atten2_conv' top: 'bl5_atten2_weight' type: "Sigmoid" }
layer { name: "bl5_b123_atten_output" bottom: "bl5_b123_output" bottom: "bl5_atten2_weight" 
	top: "bl5_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl5_res_sum" bottom: "bl5_atten2_output" bottom: "bl4_res_output" top: "bl5_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl5_res_output' bottom: 'bl5_res_sum' top: 'bl5_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl5_res_output_relu' bottom: 'bl5_res_output' top: 'bl5_res_output' type: "ReLU" }


##block 6, kernel{16, 8, 8}
#branch 1
layer { name: 'bl6_b1_conv' bottom: 'bl5_res_output' top: 'bl6_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_b1_relu' bottom: 'bl6_b1_conv' top: 'bl6_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl6_b2_pool' bottom: 'bl5_res_output' top: 'bl6_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl6_b2_conv' bottom: 'bl6_b2_pool' top: 'bl6_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_b2_relu' bottom: 'bl6_b2_conv' top: 'bl6_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl6_b3_pool' bottom: 'bl5_res_output' top: 'bl6_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl6_b3_conv' bottom: 'bl6_b3_pool' top: 'bl6_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_b3_relu' bottom: 'bl6_b3_conv' top: 'bl6_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl6_b3_up2" type: "Deconvolution"  bottom: 'bl6_b3_conv' top: 'bl6_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl6_b3_crop" type: "Crop" bottom: 'bl6_b3_up2' bottom: 'bl6_b2_conv' top: 'bl6_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl6_b23_fusion" bottom: "bl6_b3_crop" bottom: "bl6_b2_conv" top: "bl6_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl6_b23_output' bottom: 'bl6_b23_fusion' top: 'bl6_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_b23_output_relu' bottom: 'bl6_b23_output' top: 'bl6_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl6_atten1_conv' bottom: 'bl6_b23_output' top: 'bl6_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl6_atten1_relu' bottom: 'bl6_atten1_conv' top: 'bl6_atten1_conv' type: "ReLU" }
layer { name: 'bl6_atten1_weight' bottom: 'bl6_atten1_conv' top: 'bl6_atten1_weight' type: "Sigmoid" }
layer { name: "bl6_b23_atten_output" bottom: "bl6_b23_output" bottom: "bl6_atten1_weight" 
	top: "bl6_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl6_b23_output
layer { name: "bl6_b23_output_up2" type: "Deconvolution"  bottom: 'bl6_atten1_output' top: 'bl6_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl6_b23_output_up2_crop" type: "Crop" bottom: 'bl6_b23_output_up2' bottom: 'bl6_b1_conv' top: 'bl6_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl6_b123_fusion" bottom: "bl6_b23_output_up2_crop" bottom: "bl6_b1_conv" top: "bl6_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl6_b123_output' bottom: 'bl6_b123_fusion' top: 'bl6_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_b123_output_relu' bottom: 'bl6_b123_output' top: 'bl6_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl6_atten2_conv' bottom: 'bl6_b123_output' top: 'bl6_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl6_atten2_relu' bottom: 'bl6_atten2_conv' top: 'bl6_atten2_conv' type: "ReLU" }
layer { name: 'bl6_atten2_weight' bottom: 'bl6_atten2_conv' top: 'bl6_atten2_weight' type: "Sigmoid" }
layer { name: "bl6_b123_atten_output" bottom: "bl6_b123_output" bottom: "bl6_atten2_weight" 
	top: "bl6_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl6_res_sum" bottom: "bl6_atten2_output" bottom: "bl5_res_output" bottom: "subnet_score"
	top: "bl6_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl6_res_output' bottom: 'bl6_res_sum' top: 'bl6_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl6_res_output_relu' bottom: 'bl6_res_output' top: 'bl6_res_output' type: "ReLU" }


##block 7, kernel{16, 8, 8}
#branch 1
layer { name: 'bl7_b1_conv' bottom: 'bl6_res_output' top: 'bl7_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_b1_relu' bottom: 'bl7_b1_conv' top: 'bl7_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl7_b2_pool' bottom: 'bl6_res_output' top: 'bl7_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl7_b2_conv' bottom: 'bl7_b2_pool' top: 'bl7_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_b2_relu' bottom: 'bl7_b2_conv' top: 'bl7_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl7_b3_pool' bottom: 'bl6_res_output' top: 'bl7_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl7_b3_conv' bottom: 'bl7_b3_pool' top: 'bl7_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_b3_relu' bottom: 'bl7_b3_conv' top: 'bl7_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl7_b3_up2" type: "Deconvolution"  bottom: 'bl7_b3_conv' top: 'bl7_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl7_b3_crop" type: "Crop" bottom: 'bl7_b3_up2' bottom: 'bl7_b2_conv' top: 'bl7_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl7_b23_fusion" bottom: "bl7_b3_crop" bottom: "bl7_b2_conv" top: "bl7_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl7_b23_output' bottom: 'bl7_b23_fusion' top: 'bl7_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_b23_output_relu' bottom: 'bl7_b23_output' top: 'bl7_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl7_atten1_conv' bottom: 'bl7_b23_output' top: 'bl7_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl7_atten1_relu' bottom: 'bl7_atten1_conv' top: 'bl7_atten1_conv' type: "ReLU" }
layer { name: 'bl7_atten1_weight' bottom: 'bl7_atten1_conv' top: 'bl7_atten1_weight' type: "Sigmoid" }
layer { name: "bl7_b23_atten_output" bottom: "bl7_b23_output" bottom: "bl7_atten1_weight" 
	top: "bl7_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl7_b23_output
layer { name: "bl7_b23_output_up2" type: "Deconvolution"  bottom: 'bl7_atten1_output' top: 'bl7_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl7_b23_output_up2_crop" type: "Crop" bottom: 'bl7_b23_output_up2' bottom: 'bl7_b1_conv' top: 'bl7_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl7_b123_fusion" bottom: "bl7_b23_output_up2_crop" bottom: "bl7_b1_conv" top: "bl7_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl7_b123_output' bottom: 'bl7_b123_fusion' top: 'bl7_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_b123_output_relu' bottom: 'bl7_b123_output' top: 'bl7_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl7_atten2_conv' bottom: 'bl7_b123_output' top: 'bl7_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl7_atten2_relu' bottom: 'bl7_atten2_conv' top: 'bl7_atten2_conv' type: "ReLU" }
layer { name: 'bl7_atten2_weight' bottom: 'bl7_atten2_conv' top: 'bl7_atten2_weight' type: "Sigmoid" }
layer { name: "bl7_b123_atten_output" bottom: "bl7_b123_output" bottom: "bl7_atten2_weight" 
	top: "bl7_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl7_res_sum" bottom: "bl7_atten2_output" bottom: "bl6_res_output" top: "bl7_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl7_res_output' bottom: 'bl7_res_sum' top: 'bl7_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl7_res_output_relu' bottom: 'bl7_res_output' top: 'bl7_res_output' type: "ReLU" }


##block 8, kernel{16, 8, 8}
#branch 1
layer { name: 'bl8_b1_conv' bottom: 'bl7_res_output' top: 'bl8_b1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_b1_relu' bottom: 'bl8_b1_conv' top: 'bl8_b1_conv' type: "ReLU" }

#branch 2
layer { name: 'bl8_b2_pool' bottom: 'bl7_res_output' top: 'bl8_b2_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 3 stride: 2} }
layer { name: 'bl8_b2_conv' bottom: 'bl8_b2_pool' top: 'bl8_b2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_b2_relu' bottom: 'bl8_b2_conv' top: 'bl8_b2_conv' type: "ReLU" }

#branch 3
layer { name: 'bl8_b3_pool' bottom: 'bl7_res_output' top: 'bl8_b3_pool' type: "Pooling"
  pooling_param {pool: MAX kernel_size: 5 stride: 4} }
layer { name: 'bl8_b3_conv' bottom: 'bl8_b3_pool' top: 'bl8_b3_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_b3_relu' bottom: 'bl8_b3_conv' top: 'bl8_b3_conv' type: "ReLU" }

# fusion: branch 2, 3
layer { name: "bl8_b3_up2" type: "Deconvolution"  bottom: 'bl8_b3_conv' top: 'bl8_b3_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		  convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl8_b3_crop" type: "Crop" bottom: 'bl8_b3_up2' bottom: 'bl8_b2_conv' top: 'bl8_b3_crop' crop_param {axis: 2 offset: 1 }}
layer { name: "bl8_b23_fusion" bottom: "bl8_b3_crop" bottom: "bl8_b2_conv" top: "bl8_b23_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl8_b23_output' bottom: 'bl8_b23_fusion' top: 'bl8_b23_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_b23_output_relu' bottom: 'bl8_b23_output' top: 'bl8_b23_output' type: "ReLU" }

# attention: branch 23
layer { name: 'bl8_atten1_conv' bottom: 'bl8_b23_output' top: 'bl8_atten1_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 8 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl8_atten1_relu' bottom: 'bl8_atten1_conv' top: 'bl8_atten1_conv' type: "ReLU" }
layer { name: 'bl8_atten1_weight' bottom: 'bl8_atten1_conv' top: 'bl8_atten1_weight' type: "Sigmoid" }
layer { name: "bl8_b23_atten_output" bottom: "bl8_b23_output" bottom: "bl8_atten1_weight" 
	top: "bl8_atten1_output" type: "Eltwise" eltwise_param { operation: PROD}}

# fusion: branch 1, bl8_b23_output
layer { name: "bl8_b23_output_up2" type: "Deconvolution"  bottom: 'bl8_atten1_output' top: 'bl8_b23_output_up2'
	  param { lr_mult: 0 decay_mult: 1}  param { lr_mult: 0 decay_mult: 0}
		convolution_param { kernel_size: 4 stride: 2 num_output: 8  weight_filler {type:"bilinear"}} }
layer { name: "bl8_b23_output_up2_crop" type: "Crop" bottom: 'bl8_b23_output_up2' bottom: 'bl8_b1_conv' top: 'bl8_b23_output_up2_crop' crop_param {axis: 2 offset: 1 }}

layer { name: "bl8_b123_fusion" bottom: "bl8_b23_output_up2_crop" bottom: "bl8_b1_conv" top: "bl8_b123_fusion" type: "Concat" concat_param { concat_dim: 1 }}
layer { name: 'bl8_b123_output' bottom: 'bl8_b123_fusion' top: 'bl8_b123_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_b123_output_relu' bottom: 'bl8_b123_output' top: 'bl8_b123_output' type: "ReLU" }

# attention: branch 123
layer { name: 'bl8_atten2_conv' bottom: 'bl8_b123_output' top: 'bl8_atten2_conv' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 stride: 1 weight_filler{type:"xavier"}} }
layer { name: 'bl8_atten2_relu' bottom: 'bl8_atten2_conv' top: 'bl8_atten2_conv' type: "ReLU" }
layer { name: 'bl8_atten2_weight' bottom: 'bl8_atten2_conv' top: 'bl8_atten2_weight' type: "Sigmoid" }
layer { name: "bl8_b123_atten_output" bottom: "bl8_b123_output" bottom: "bl8_atten2_weight" 
	top: "bl8_atten2_output" type: "Eltwise" eltwise_param { operation: PROD}}

layer { name: "bl8_res_sum" bottom: "bl8_atten2_output" bottom: "bl7_res_output" bottom: "subnet_score"
	top: "bl8_res_sum" type: "Concat" concat_param {concat_dim: 1}}
layer { name: 'bl8_res_output' bottom: 'bl8_res_sum' top: 'bl8_res_output' type: "Convolution"
  param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0}
  convolution_param { num_output: 16 pad: 1  kernel_size: 3 weight_filler{type:"xavier"}} }
layer { name: 'bl8_res_output_relu' bottom: 'bl8_res_output' top: 'bl8_res_output' type: "ReLU" }


## supervision loss
layer { name: 'score1' type: "Convolution" bottom: 'bl2_res_output' top: 'score1'
  param { lr_mult: 0.1 decay_mult: 1 } param { lr_mult: 0.2 decay_mult: 0}
  convolution_param { num_output: 1 kernel_size: 1 weight_filler {type:"xavier"} } }
layer { type: "SigmoidWeightedCrossEntropyLoss" bottom: "score1" bottom: "label" top: "loss1" loss_weight: 1 }

layer { name: 'score2' type: "Convolution" bottom: 'bl4_res_output' top: 'score2'
  param { lr_mult: 0.1 decay_mult: 1 } param { lr_mult: 0.2 decay_mult: 0}
  convolution_param { num_output: 1 kernel_size: 1 weight_filler {type:"xavier"} } }
layer { type: "SigmoidWeightedCrossEntropyLoss" bottom: "score2" bottom: "label" top: "loss2" loss_weight: 1 }

layer { name: 'score3' type: "Convolution" bottom: 'bl6_res_output' top: 'score3'
  param { lr_mult: 0.1 decay_mult: 1 } param { lr_mult: 0.2 decay_mult: 0}
  convolution_param { num_output: 1 kernel_size: 1 weight_filler {type:"xavier"} } }
layer { type: "SigmoidWeightedCrossEntropyLoss" bottom: "score3" bottom: "label" top: "loss3" loss_weight: 1 }

layer { name: 'score4' type: "Convolution" bottom: 'bl8_res_output' top: 'score4'
  param { lr_mult: 0.1 decay_mult: 1 } param { lr_mult: 0.2 decay_mult: 0}
  convolution_param { num_output: 1 kernel_size: 1 weight_filler {type:"xavier"} } }
layer { type: "SigmoidWeightedCrossEntropyLoss" bottom: "score4" bottom: "label" top: "loss4" loss_weight: 1 }
